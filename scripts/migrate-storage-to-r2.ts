/**
 * Supabase Storage â†’ Cloudflare R2 ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
 *
 * ä½¿ç”¨æ–¹æ³•:
 *   npx tsx scripts/migrate-storage-to-r2.ts
 *
 * ç’°å¢ƒå¤‰æ•°ãŒå¿…è¦:
 *   - SUPABASE_URL
 *   - SUPABASE_SERVICE_ROLE_KEY
 *   - R2_ACCOUNT_ID
 *   - R2_ACCESS_KEY_ID
 *   - R2_SECRET_ACCESS_KEY
 *   - R2_BUCKET_NAME
 *
 * ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
 *   --dry-run    å®Ÿéš›ã«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã›ãšã€å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¡¨ç¤º
 *   --bucket     ç‰¹å®šã®ãƒã‚±ãƒƒãƒˆã®ã¿ç§»è¡Œ (profiles, practices, competitions)
 */

import { createClient } from '@supabase/supabase-js'
import { S3Client, PutObjectCommand, HeadObjectCommand } from '@aws-sdk/client-s3'

// ç’°å¢ƒå¤‰æ•°ã¯ dotenvx ãŒè‡ªå‹•æ³¨å…¥ï¼ˆapps/web/.env.localï¼‰

// ç§»è¡Œå¯¾è±¡ã®ãƒã‚±ãƒƒãƒˆï¼ˆSupabase Storage ã®ãƒã‚±ãƒƒãƒˆåï¼‰
const BUCKETS = ['profile-images', 'practice-images', 'competition-images'] as const
type BucketName = (typeof BUCKETS)[number]

// å¼•æ•°ãƒ‘ãƒ¼ã‚¹
const args = process.argv.slice(2)
const isDryRun = args.includes('--dry-run')
const bucketArg = args.find(arg => arg.startsWith('--bucket='))
const targetBucket = bucketArg ? bucketArg.split('=')[1] as BucketName : null

// ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
function checkEnvVars(): void {
  const required = [
    'NEXT_PUBLIC_SUPABASE_URL',
    'SUPABASE_SERVICE_ROLE_KEY',
    'R2_ACCOUNT_ID',
    'R2_ACCESS_KEY_ID',
    'R2_SECRET_ACCESS_KEY',
    'R2_BUCKET_NAME',
  ]

  const missing = required.filter(key => !process.env[key])
  if (missing.length > 0) {
    console.error('âŒ ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“:')
    missing.forEach(key => console.error(`   - ${key}`))
    process.exit(1)
  }
}

// Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
function getSupabaseClient() {
  return createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_ROLE_KEY!
  )
}

// R2 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
function getR2Client(): S3Client {
  return new S3Client({
    region: 'auto',
    endpoint: `https://${process.env.R2_ACCOUNT_ID}.r2.cloudflarestorage.com`,
    credentials: {
      accessKeyId: process.env.R2_ACCESS_KEY_ID!,
      secretAccessKey: process.env.R2_SECRET_ACCESS_KEY!,
    },
  })
}

// R2ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
async function existsInR2(r2Client: S3Client, key: string): Promise<boolean> {
  try {
    await r2Client.send(
      new HeadObjectCommand({
        Bucket: process.env.R2_BUCKET_NAME!,
        Key: key,
      })
    )
    return true
  } catch {
    return false
  }
}

// ãƒ•ã‚¡ã‚¤ãƒ«ã®Content-Typeã‚’æ¨æ¸¬
function getContentType(filename: string): string {
  const ext = filename.split('.').pop()?.toLowerCase()
  const types: Record<string, string> = {
    jpg: 'image/jpeg',
    jpeg: 'image/jpeg',
    png: 'image/png',
    gif: 'image/gif',
    webp: 'image/webp',
    avif: 'image/avif',
    svg: 'image/svg+xml',
  }
  return types[ext || ''] || 'application/octet-stream'
}

// ãƒã‚±ãƒƒãƒˆå†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—
async function listAllFiles(
  supabase: ReturnType<typeof getSupabaseClient>,
  bucket: BucketName,
  folder: string = ''
): Promise<string[]> {
  const files: string[] = []

  const { data, error } = await supabase.storage.from(bucket).list(folder, {
    limit: 1000,
  })

  if (error) {
    console.error(`âŒ ${bucket}/${folder} ã®ä¸€è¦§å–å¾—ã«å¤±æ•—:`, error.message)
    return files
  }

  for (const item of data || []) {
    const itemPath = folder ? `${folder}/${item.name}` : item.name

    if (item.id === null) {
      // ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆã¯å†å¸°
      const subFiles = await listAllFiles(supabase, bucket, itemPath)
      files.push(...subFiles)
    } else {
      // ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
      files.push(itemPath)
    }
  }

  return files
}

// 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»è¡Œ
async function migrateFile(
  supabase: ReturnType<typeof getSupabaseClient>,
  r2Client: S3Client,
  bucket: BucketName,
  filePath: string
): Promise<{ success: boolean; skipped: boolean; error?: string }> {
  const r2Key = `${bucket}/${filePath}`

  // æ—¢ã«R2ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
  const exists = await existsInR2(r2Client, r2Key)
  if (exists) {
    return { success: true, skipped: true }
  }

  if (isDryRun) {
    return { success: true, skipped: false }
  }

  // Supabase Storageã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
  const { data, error } = await supabase.storage.from(bucket).download(filePath)

  if (error) {
    return { success: false, skipped: false, error: error.message }
  }

  // R2ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
  const buffer = Buffer.from(await data.arrayBuffer())
  const contentType = getContentType(filePath)

  try {
    await r2Client.send(
      new PutObjectCommand({
        Bucket: process.env.R2_BUCKET_NAME!,
        Key: r2Key,
        Body: buffer,
        ContentType: contentType,
      })
    )
    return { success: true, skipped: false }
  } catch (err) {
    return { success: false, skipped: false, error: String(err) }
  }
}

// ãƒã‚±ãƒƒãƒˆã‚’ç§»è¡Œ
async function migrateBucket(
  supabase: ReturnType<typeof getSupabaseClient>,
  r2Client: S3Client,
  bucket: BucketName
): Promise<{ total: number; migrated: number; skipped: number; failed: number }> {
  console.log(`\nğŸ“¦ ${bucket} ãƒã‚±ãƒƒãƒˆã®ç§»è¡Œã‚’é–‹å§‹...`)

  const files = await listAllFiles(supabase, bucket)
  console.log(`   ${files.length} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º`)

  let migrated = 0
  let skipped = 0
  let failed = 0

  for (const filePath of files) {
    const result = await migrateFile(supabase, r2Client, bucket, filePath)

    if (result.success) {
      if (result.skipped) {
        skipped++
        process.stdout.write('.')
      } else {
        migrated++
        process.stdout.write('âœ“')
      }
    } else {
      failed++
      console.error(`\n   âŒ ${filePath}: ${result.error}`)
    }
  }

  console.log(`\n   å®Œäº†: ç§»è¡Œ=${migrated}, ã‚¹ã‚­ãƒƒãƒ—=${skipped}, å¤±æ•—=${failed}`)

  return { total: files.length, migrated, skipped, failed }
}

// ãƒ¡ã‚¤ãƒ³å‡¦ç†
async function main() {
  console.log('='.repeat(60))
  console.log('Supabase Storage â†’ Cloudflare R2 ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
  console.log('='.repeat(60))

  if (isDryRun) {
    console.log('ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯è¡Œã„ã¾ã›ã‚“')
  }

  if (targetBucket) {
    if (!BUCKETS.includes(targetBucket as BucketName)) {
      console.error(`âŒ ç„¡åŠ¹ãªãƒã‚±ãƒƒãƒˆå: ${targetBucket}`)
      console.error(`   æœ‰åŠ¹ãªãƒã‚±ãƒƒãƒˆ: ${BUCKETS.join(', ')}`)
      process.exit(1)
    }
    console.log(`ğŸ“ å¯¾è±¡ãƒã‚±ãƒƒãƒˆ: ${targetBucket}`)
  } else {
    console.log(`ğŸ“ å¯¾è±¡ãƒã‚±ãƒƒãƒˆ: ${BUCKETS.join(', ')}`)
  }

  checkEnvVars()

  const supabase = getSupabaseClient()
  const r2Client = getR2Client()

  const bucketsToMigrate = targetBucket ? [targetBucket as BucketName] : BUCKETS

  const summary = {
    total: 0,
    migrated: 0,
    skipped: 0,
    failed: 0,
  }

  for (const bucket of bucketsToMigrate) {
    const result = await migrateBucket(supabase, r2Client, bucket)
    summary.total += result.total
    summary.migrated += result.migrated
    summary.skipped += result.skipped
    summary.failed += result.failed
  }

  console.log('\n' + '='.repeat(60))
  console.log('ç§»è¡Œå®Œäº†ã‚µãƒãƒªãƒ¼')
  console.log('='.repeat(60))
  console.log(`   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: ${summary.total}`)
  console.log(`   ç§»è¡Œæ¸ˆã¿: ${summary.migrated}`)
  console.log(`   ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰: ${summary.skipped}`)
  console.log(`   å¤±æ•—: ${summary.failed}`)

  if (summary.failed > 0) {
    console.log('\nâš ï¸  ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç§»è¡Œã«å¤±æ•—ã—ã¾ã—ãŸ')
    process.exit(1)
  }

  if (isDryRun) {
    console.log('\nâœ… ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†ã€‚å®Ÿéš›ã«ç§»è¡Œã™ã‚‹ã«ã¯ --dry-run ã‚’å¤–ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„')
  } else {
    console.log('\nâœ… ç§»è¡ŒãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ')
  }
}

main().catch(err => {
  console.error('âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼:', err)
  process.exit(1)
})
